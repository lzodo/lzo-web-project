<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<title>Document</title>
	<script src="https://cdn.bootcdn.net/ajax/libs/spark-md5/3.0.2/spark-md5.js"></script>
	<script src="./static/spark-md5.js"></script>
</head>

<body>
	<input id='fileInput' type="file" accept="image/*" name="file" />
	<div class="container">
		<div class="inner"></div>
	</div>
</body>
<script>
	/**
	 * 1、将 file 文件 通过 slice 进行切片生成很多个小的 blob 对象
	 * 2、获取文件 hash，作为文件的唯一识别码，下次传的时候，询问后端这个hash的文件传到哪里，后端返回一个编号，前端继续从这个切片位置续传
	 * 		2-1、可以通过 spark-md5 生成文件hash
	 *      2-2、npm install --save spark-md5 或 cdn 安装
	 *
	 * 概念
	 *   File，文件类型input可以得到一组文件对象files，这个类数组每一项都是一个File对象，存储着我们选择的文件
	 *   Blob，表示二进制数据，File的父类，
	 *   FormData，存储文件信息，通过二进制形式传递给后端
	 *
	 * 前端发起上传后，后端部分：
	 *   1、后端根据前端的md5前面查询文件是否存在
	 * 		-	存在-- 实现秒传
	 *      -   部分存在-- 上传不存在的切片（断点续传）
	 *      -   不存在 -- 切片上传
	 *   2、后端拿到上传的文件(片)
	 *      -   计算文件（片）数据
	 *      -   存储文件（片）（本地存储，或第三方存储库）
	 *   3、判断是否存储成功
	 *      -   成功 -- 保存文件（片）信息
	 *      -   失败 -- 提示失败
	 *    4、如果存储成功了，判断是否是最后一片
	 *      -   是 -- 合并文件 -- 保存完整文件信息，提示成功，结束流程
	 *      -   否 -- 继续上传
	 *
	 */
	window.onload = function () {
		let input = document.querySelector("#fileInput")
		input.addEventListener('change', handleFile)
	};

	async function handleFile(e) {
		let file = [...e.target.files][0];
		if (!file) return

		// 得到所有切片
		let chunks = pieceChunks(file, 20000);
		console.log(chunks, 111);


		// 计算hash
		let hash = await getHash(chunks)
		console.log(hash, 222)

		// 需要的话，可以用web worker 单独开一个线程避免浏览器卡死（这边先不用）
		chunks.map(item => {
			const formData = new FormData();
			formData.append('files', item);
			formData.append('hash', hash);
			// formData.append('filename', fileName);
			uploadFile(formData) // 后端接口还没写
		})

		// 创建 FormData 通过接口提交给后端
		// let fileData = new FormData();
		// fileData.append('files', file);


		// uploadFile(fileData)

	}

	// 切片函数
	function pieceChunks(file, chunkSize) {
		console.log(file);

		let result = []
		for (let i = 0; i < file.size; i += chunkSize) {
			// 拿到 0-99 个字节的切片，得到一个blob对象
			// const piece = file.slice(0, 100)
			let piece = file.slice(i, i + chunkSize);
			result.push(piece)
		}
		return result
	}

	// 计算 hash，增量风快计算，计算完一块，从内存中清除，再计算下一块
	// 计算这个文件的hash值
	function getHash(chunks) {
		return new Promise((resolve, reject) => {
			var spark = new SparkMD5(); // 将任何数据换算成一个固定长度的字符串
			// 读取切片
			function _read(i) {
				// 递归终止条件
				if (i >= chunks.length) {
					// 全部执行完成之后
					resolve(spark.end())
					return;
				}
				// 拿到分块
				const blob = chunks[i]

				// 读取分块字节数组,FileReader可以这阵的读取出分片文件数据，用这个生产hash确保唯一性
				const reader = new FileReader();
				reader.readAsArrayBuffer(blob)
				reader.onload = e => {
					// 得到该分片的二进制字节数组
					const bytes = e.target.result;
					console.log(bytes, 'bytes');

					// 将结果push到 spark-md5 的 spark中（只是加入计算的作用）
					spark.append(bytes)
					// 递归调用自生
					_read(i + 1)
				}
			}

			// 从第0个开始递归读取
			_read(0)
		})
	}

	//api 提交上传
	function uploadFile(fileData) {
		fetch('http://localhost:8778/uploads/chunks', {
			method: 'post',
			body: fileData,
		})
			.then(function (response) {
				return response.json();
			})
			.then(function (data) {
				console.log(data);
			})
			.catch(function (error) {
				console.log(error);
			});
	}
</script>
<style>
	* {
		margin: 0;
		padding: 0;
		box-sizing: border-box;
	}

	html,
	body {
		width: 100%;
		height: 100%;
	}

	/* 容器 */
	.container {
		height: 100%
	}

	.container .inner {}
</style>

</html>